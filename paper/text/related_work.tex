\section{Related Work}
\label{sec:related_work}

There have been numerous distributed file systems, so this project shares a lot in common with several past research endeavors.  However, the three projects that served as the biggest inspirations for SPOOFS are the Hadoop File System, Ceph, and the Google File System.

The Hadoop File System (HDFS) \cite{borthakur_hdfs09} is the file system that underlies Hadoop, an open-source Map/Reduce implementation.  HDFS is set up in a very traditional Client-Master-Storage architecture.  In this architecture, there is one central Master node that holds all the information about the distributed file system and through which Clients are able to access data.  Storage nodes store the actual data, often holding parts of files.

In HDFS, the master is called the \textit{NameNode} and each storage server is called a \textit{DataNode}.  Many of the design decisions for HDFS were made with its end application, Hadoop, in mind.  For instance, HDFS is written in Java and built as an application-level file system so that it can be accessed from a Hadoop application, also written in Java.  In Hadoop, the mantra of ``moving computation is cheaper than moving data'' means that HDFS provides features that can move computation to a particular \textit{DataNode}.  Another aspect of HDFS worth noting is that only one \textit{NameNode} exists in the system.  This acts as a single-point-of-failure and a crash on the \textit{NameNode} can potentially bring down the whole system.

Ceph \cite{weil_osdi06} is a distributed file system written with scalability, performance, and reliability in mind.  Written as a FUSE module for the Linux kernel, Ceph is meant to provide more general-purpose functionality to a variety of systems.  Ceph still maintains the typical client and storage nodes, but replaces the master server with a metadata cluster.  The goal of the metadata cluster is to dynamically distribute the metadata in a way that can scale to higher numbers of clients and potentially yield better performance.  The authors admit that this makes consistency more difficult to maintain, but it provides the added benefit of avoiding a single-point-of-failure unlike HDFS.

The Google File System (GFS) \cite{ghemawat_sigops03} is used by several distributed applications at Google, including Bigtable \cite{chang_osdi06}.  Like HDFS, it also follows the typical Client-Master-Storage architecture, but adds some more guarantees for fault tolerance.  Unlike a job in Hadoop, which generally has a beginning and end, the applications utilizing GFS need constant uptime because they cannot fully predict when they will be used.  This means that consistency and fault tolerance are a priority for GFS.  To alleviate recovery upon master failure, GFS uses a journaling scheme similar to many traditional file systems \cite{seltzer_atc00}.  Journaling, in the case of GFS, means writing a persistent journal both locally \textit{and} on a backup computer.  This way, upon master failure, the backup node can replay the journal to recover to a consistent file system state.

