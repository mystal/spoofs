\section{SPOOFS}
\label{sec:spoofs}

In this paper we present SPOOFS: a SimPle, Object-Oriented, File System. This is our take on a distributed file system and, though at an early stage, it is fully capable of servicing read, write, and append requests from connected clients. The architecture is the standard Client-Master-Storage architecture, with one Master node and several Storage and Client nodes, similar to HDFS and GFS. The entire codebase is written in Java---chosen for its ease of development and use of by many other distributed systems---and handles communication through Java object serialization.

While designing and writing SPOOFS, we aimed to achieve a couple of basic goals:
\begin{enumerate}
\item A simple and working read/write/append distributed file system with
\item The ability to add/remove storage and client nodes dynamically and
\item A clean interface for clients to interact with the file system
\end{enumerate}
Our main goal, however, was in addressing the question, ``What happens if the Master node goes down?'' Some distributed file systems, notably HDFS, do not have particularly elegant methods for handling Master failure. In fact, in the case of HDFS the entire system will go down and only after performing a rather lengthy recovery process may the file system be brought up and used again. Therefore, we strove to provide a mechanism to seamlessly recover the file system if the Master were to instantly disappear from the distributed system---for example, if the power cord had been accidentally pulled out of the machine.

For the sake of simplicity---our group had only a few months to work on the project---files are not split into chunks but are instead stored as a single file on a Storage node. In addition, the files are not currently replicated across several Storage nodes, though the functionality can be added. \todo{Any more?}

\todo{Include figure of architecture}

\subsection{Master Node}

The Master node in our system is, in the traditional sense, the master of the entire file system, watching over nodes connected to the system. In addition, it is the first node that must be brought online for SPOOFS to function and also the central node responsible for handling all metadata operations. Storage nodes that are added and clients that connect to the file system interact with the master at initialization and whenever performing operations that modify the layout of the file system.

At this time, the Master stores only a few pieces of metadata:
\begin{enumerate}
\item Directory and file hierarchy
\item Physical location (IP address) of files
\item Storage nodes online
\item Clients connected
\end{enumerate}
The directory and file hierarchy are represented in the same way as on traditional UNIX-based systems (e.g. ``/foo/bar.txt''). When a file is created, the Master does a round-robin search for an available Storage node. Upon finding a valid candidate, it will then tell the node to locally create the file and return with a response once it has completed the task. At this point, the Master can store a map of a string representation of the location of the file in the distributed file system to the IP address of the Storage node containing it. Doing a search, then, is a trivial task.

Storage nodes are stored in a map of an assigned identification number to IP address. Keeping track of the online Storage nodes is used for knowing when a Storage node has been taken offline. By exchanging \textit{KeepAlive} communication between the Master and Storage nodes at a set interval, the Master can assume that a Storage node has gone down after missing a few \textit{KeepAlives}. At such a point, it can choose how to respond, possibly by removing the files on that Storage node. However, we have not yet decided how to handle this case.

Clients, like Storage nodes, are also assigned identification numbers. Having a list of connected Clients allows the Master node to also exchange \textit{KeepAlives} with them. \todo{more about stuff we can do with clients}

The Master currently stores all of this data in memory; it is not flushed to disk. \todo{any other miscellaneous things?}

\subsection{Storage Nodes}

Storage nodes in SPOOFS store the actual data of the distributed file system. They can be dynamically brought online and taken offline once the Master node has been initialized. Upon connecting to the Master, a Storage node will perform a handshake, giving the Master a port to send communication to and receiving an identification number that it is to send with its \textit{KeepAlives}.

When given a file to store, a Storage node will also keep track of the location of the file in the distributed file system. This is used for the recovery of the file system.

Handling synchronization of readers and writers is done entirely on Storage nodes through the use of a Java ReentrantReadWriteLock. This can be done as single files in the distributed file system are stored in one location, one file in a Storage node, of the entire system. That and the fact that Clients need not go through the Master once they have opened a file.

\subsection{Client Nodes and Interface}

Client nodes interact with SPOOFS through a session-based model supported by a Client library package we developed. A session is started through the \texttt{connect} call, which begins a handshake procedure with the Master node. Once this channel has been established, the Client begins sending \textit{KeepAlives} with its assigned identification number at a set interval so that the Master may track it.

The library exposed to the Client contains an \textit{SClient} class. Instances of the class may be used to connect to a SPOOFS Master and support the operations \texttt{createFile}, \texttt{createDirectory}, \texttt{open}, \texttt{close}, \texttt{removeFile}, \texttt{removeDirectory}, \texttt{read}, \texttt{write}, and \texttt{append}. These do as their names imply. The \texttt{createFile} and \texttt{open} calls return a reference to an \textit{SFile} object, representing the newly created or opened file. These objects act as file handles for SPOOFS, storing the IP address and port of the Storage node containing the file in addition to the Client's current position in the file, and are required to be passed in by several of the other Client operations. Also, references to open \textit{SFile} objects are cached locally in an active \textit{SClient} object.

The only calls requiring direct intervention by the Master are \texttt{open} and those that modify the file system layout: \texttt{createFile}, \texttt{createDirectory}, \texttt{removeFile}, \texttt{removeDirectory}. The others require an \textit{SFile} object to be passed in, which is then used to locate the file on the Storage node and perform the operation.

To finish a session the \textit{disconnect} call is made, which will close all open \textit{SFile} objects and unregister from the Master.

\subsection{Backup and Recovery}

One of the design goals for SPOOFS was to answer the question ``What happens if the Master node goes down?'' with a resounding answer, ``The system will recover and continue to make progress.''  The question itself is open to interpretation, but the failure model that we chose is one in which the master node crashes or is brought offline completely.  Several scenarios, including malicious attacks on the Master, power outage, or network outage make this failure model possible.  The recovery process in SPOOFS cannot guarantee 100\% consistency in all failures, but can recover successfully from our failure model.

The first step in answering our question is to provide a backup to the Master node in some manner.  This is accomplished through the \textit{MasterBackup} node.  This node can be brought online at any time, though ideally it will be brought on early in the life of a \textit{MasterServer} in case of failure.  The \textit{MasterBackup} first initiates contact with the Master node.  It alerts the master of its existence, and provides its IP address and port on which it can be ``reached'' for any future requests.  After hearing back an \texttt{OK} from the Master node, it starts up its own KeepAlive client, which acts similar to all other KeepAlive clients, pulsing at a regular interval.

Once the Backup is online, the Master node periodically sends \textit{BackupRequest} messages to the Backup node.  These \textit{BackupRequest} objects contain information on all current nodes in the system, whether they have joined or have left the system.  The Backup node stores this information internally, and it represents a mirrored copy of the information that is on the Master node.  The \textit{BackupRequest} objects are currently buffered on the Master node for a short amount of time before being written to the Backup node for performance reasons.  This feature, however, is tunable to potentially become a write-though operation to guarantee better consistency.

Now, assume that the Master node crashes and goes offline.  The \textit{MasterBackup} will send a KeepAlive request at some point, which will timeout after a given interval.  When this interval is reached, the \textit{MasterBackup} determines that the master has crashed and goes into Recovery mode.  The first thing that it does it start up a new \textit{MasterServer} instance on its node.  This instance has all the functionality off the old Master node.  Then, it iterates through the list of all Storage nodes in the system, and broadcasts its existence to them.  Another option would possibly have been to reclaim the name of the old server (i.e. - the Backup node becomes ``master'').  We chose against this to avoid having to include a naming service into our design.

After the new Master node has broadcast its presence, each Storage node sends the master a snapshot of the files it currently has.  So, for instance, Storage node 0 would tell the new Master node that it contains the SPOOFS file ``/foo/bar.txt''.  The new Master node handles all incoming requests and maps the files to their respective Storage node.  After all Storage nodes have reported back, the new Master then alerts its presence to all clients.  After they are aware of the new Master node, it then starts up the KeepAlive handler and the system is recovered.  Note, however, that clients with previously opened file handles can continue to make progress while the Master node is down by communicating directly with the Storage node.  Only those requests that require opening a new file handle will suffer temporary failure while the new Master is getting itself online.
