\section{SPOOFS}
\label{sec:spoofs}

In this paper we present SPOOFS: a SimPle, Object-Oriented, File System. This is our take on a distributed file system and, though at an early stage, it is fully capable of servicing read, write, and append requests from connected clients. The architecture is the standard Client-Master-Storage architecture, with one Master node and several Storage and Client nodes, similar to that of HDFS and GFS. It is written in Java, chosen for its ease of development and use of by many other distributed systems, and handles communication through Java object serialization.

For SPOOFS, we aimed to achieve a couple of basic goals:
\begin{enumerate}
\item A simple and working read/write/append distributed file system with
\item The ability to add/remove storage and client nodes dynamically and
\item one more thing
\end{enumerate}
Our main goal, however, was in addressing the question, "What happens if the Master node goes down?" As mentioned above, HDFS do not have particularly elegant methods for handling Master failure. In fact, in the case of HDFS the entire system will go down and then only after a rather lengthy recovery process may the file system be brought up and used again. Therefore, we strove to provide a mechanism to seamlessly recover the file system if the Master were to instantly disappear from the distributed system---for example, if the power cord had been accidentally pulled out of the machine.

For the sake of simplicity---our group had only a few months to work on the project---files are not split into chunks but are instead stored as a single file on a Storage node. In addition, the files are not currently replicated across several Storage nodes, though the functionality can be added. \todo{Any more?}

\subsection{Master Node}

The Master node in our system is represented by the MasterServer class. It is the first node that must be brought online for SPOOFS to function and also the central node responsible for handling metadata operations on the file system. Storage nodes and clients that connect to the file system interact with the master at initialization and whenever performing operations that modify the layout of the file system.

At this time, the Master stores only a few pieces of metadata:
\begin{enumerate}
\item Directory and file hierarchy
\item Physical location (IP address) of files
\item Storage nodes online
\item Client nodes online
\end{enumerate}

\todo{more general stuff about it? such as the assumptions we made: one file on one storage node per distributed file, files assigned to storage nodes in a round-robin fashion, we're really close to getting it to work need to support storage nodes dropping out and replication of data and possibly striping data in blocks across storage nodes}

\subsection{Storage Nodes}

text

\subsection{Client Nodes and Interface}

text

\subsection{Backup and Recovery}

One of the design goals for SPOOFS was to answer the question ``What happens if the Master node goes down?'' with a resounding answer, ``The system will recover and continue to make progress.''  The question itself is open to interpretation, but the failure model that we chose is one in which the master node crashes or is brought offline completely.  Several scenarios, including malicious attacks on the Master, power outage, or network outage make this failure model possible.  The recovery process in SPOOFS cannot guarantee 100\% consistency in all failures, but can recover successfully from our failure model.

The first step in answering our question is to provide a backup to the Master node in some manner.  This is accomplished through the \textit{MasterBackup} node.  This node can be brought online at any time, though ideally it will be brought on early in the life of a \textit{MasterServer} in case of failure.  The \textit{MasterBackup} first initiates contact with the Master node.  It alerts the master of its existence, and provides its IP address and port on which it can be ``reached'' for any future requests.  After hearing back an \texttt{OK} from the Master node, it starts up its own KeepAlive client, which acts similar to all other KeepAlive clients, pulsing at a regular interval.

Once the Backup is online, the Master node periodically sends \textit{BackupRequest} messages to the Backup node.  These \textit{BackupRequest} objects contain information on all current nodes in the system, whether they have joined or have left the system.  The Backup node stores this information internally, and it represents a mirrored copy of the information that is on the Master node.  The \textit{BackupRequest} objects are currently buffered on the Master node for a short amount of time before being written to the Backup node for performance reasons.  This feature, however, is tunable to potentially become a write-though operation to guarantee better consistency.

Now, assume that the Master node crashes and goes offline.  The \textit{MasterBackup} will send a KeepAlive request at some point, which will timeout after a given interval.  When this interval is reached, the \textit{MasterBackup} determines that the master has crashed and goes into Recovery mode.  The first thing that it does it start up a new \textit{MasterServer} instance on its node.  This instance has all the functionality off the old Master node.  Then, it iterates through the list of all Storage nodes in the system, and broadcasts its existence to them.  Another option would possibly have been to reclaim the name of the old server (i.e. - the Backup node becomes ``master'').  We chose against this to avoid having to include a naming service into our design.

After the new Master node has broadcast its presence, each Storage node sends the master a snapshot of the files it currently has.  So, for instance, Storage node 0 would tell the new Master node that it contains the SPOOFS file ``/foo/bar.txt''.  The new Master node handles all incoming requests and maps the files to their respective Storage node.  After all Storage nodes have reported back, the new Master then alerts its presence to all clients.  After they are aware of the new Master node, it then starts up the KeepAlive handler and the system is recovered.  Note, however, that clients with previously opened file handles can continue to make progress while the Master node is down by communicating directly with the Storage node.  Only those requests that require opening a new file handle will suffer temporary failure while the new Master is getting itself online.